{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geospatial Analysis with Python - final task**\n",
    "\n",
    "This document will cover the tasks assigned in the document [\\\\Users\\\\tmi\\\\Python\\\\GeoSpatFinalTask\\final.pdf](C:\\\\Users\\\\tmi\\\\Python\\\\GeoSpatFinalTask\\final.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by importing the necessary packages\n",
    "\n",
    "import os\n",
    "import rasterio as rio\n",
    "import matplotlib as mlp\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "from numpy import newaxis\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pysal.lib import weights\n",
    "from pysal.explore import esda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the import of necessary packages, we proceed to download the GeoTIFF file, as prompted in the task document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We proceed to import the required files. All files are from: Jones, B., and B. C. O'Neill. 2017. \n",
    "Global Population Projection Grids Based on Shared Socioeconomic Pathways (SSPs), 2010-2100. \n",
    "Palisades, NY: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/H4RF5S0P. \n",
    "Accessed 26/04/2019\n",
    "Countries.tif is imported to determine the country, which we will proceed using for our calculation\n",
    "The urban and total arrays are set up to contain all the files from 2010-2100. This is to ease the \n",
    "comparison and plotting of the contents.\n",
    "'''\n",
    "urban = ['ssp4urb2010.tif', \n",
    "         'ssp4urb2020.tif', \n",
    "         'ssp4urb2030.tif', \n",
    "         'ssp4urb2040.tif', \n",
    "         'ssp4urb2050.tif', \n",
    "         'ssp4urb2060.tif', \n",
    "         'ssp4urb2070.tif', \n",
    "         'ssp4urb2080.tif', \n",
    "         'ssp4urb2090.tif', \n",
    "         'ssp4urb2100.tif']\n",
    "\n",
    "total = ['ssp4_2010.tif', \n",
    "         'ssp4_2020.tif', \n",
    "         'ssp4_2030.tif', \n",
    "         'ssp4_2040.tif', \n",
    "         'ssp4_2050.tif', \n",
    "         'ssp4_2060.tif', \n",
    "         'ssp4_2070.tif', \n",
    "         'ssp4_2080.tif', \n",
    "         'ssp4_2090.tif', \n",
    "         'ssp4_2100.tif']\n",
    "\n",
    "countries = rio.open('countries.tif') \n",
    "#Here we open the file containing information on the countries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = {} #We set up a dictionary for the total population\n",
    "cwd = os.getcwd()\n",
    "for filename in total:\n",
    "    with rio.open(filename, 'r') as src:\n",
    "        filename = os.path.basename(filename)[:-4] \n",
    "        #Here the filenames are shortened,\n",
    "        #and we remove the need for the file extension\n",
    "        T[filename] = src.read(1)  \n",
    "        #We do not require the depth of the layer to perform our calculations - \n",
    "        #this is removed here. For dictionaries, the filenames are appended automatically, \n",
    "        #as each new input is a new line in the dictionary\n",
    "        \n",
    "U = {} #We perform the same operations as above, however this time for the Urban population\n",
    "for filename in urban:\n",
    "    with rio.open(filename, 'r') as src:\n",
    "        filename = os.path.basename(filename)[:-4]\n",
    "        U[filename] = src.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1a\n",
    "\n",
    "For this assignment, I will focus on the country (Island nation) of Palau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data for shapefile of the world comes from naturalearthdata.com\n",
    "'''\n",
    "\n",
    "country = countries.read(1) #Choosing the layer of the file, which contains the numbers of the countries\n",
    "\n",
    "Palau = country == 585 # We define the country code for Palau, to determine the coordinates from the layer\n",
    "Palau1 = np.where(Palau == 1) # Here we choose only the cells, which are true for having code 585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the sum of the urban population for each decade from 2010 to 2100 for the country of Palau. \n",
    "#This is done by performing a summation for all the cells determined as Palau with the matching cells \n",
    "#in the individual decade file.\n",
    "upop10 = np.sum(U['ssp4urb2010'][Palau1])\n",
    "upop20 = np.sum(U['ssp4urb2020'][Palau1])\n",
    "upop30 = np.sum(U['ssp4urb2030'][Palau1])\n",
    "upop40 = np.sum(U['ssp4urb2040'][Palau1])\n",
    "upop50 = np.sum(U['ssp4urb2050'][Palau1])\n",
    "upop60 = np.sum(U['ssp4urb2060'][Palau1])\n",
    "upop70 = np.sum(U['ssp4urb2070'][Palau1])\n",
    "upop80 = np.sum(U['ssp4urb2080'][Palau1])\n",
    "upop90 = np.sum(U['ssp4urb2090'][Palau1])\n",
    "upop100 = np.sum(U['ssp4urb2100'][Palau1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the arrays for the plotting the urban population\n",
    "\n",
    "Urban = [upop10,upop20,upop30,upop40,upop50,upop60,upop70,upop80,upop90,upop100] #Our y-axis, containing the population\n",
    "Year = [2010,2020,2030,2040,2050,2060,2070,2080,2090,2100] #Our x-axis, defining the years\n",
    "plt.plot(Year,Urban) \n",
    "plt.ylabel('Urban Population') #Naming the y-axis\n",
    "plt.xlabel('Year') #Naming the x-axis\n",
    "plt.title('Palau') #Giving the chart a title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the urban population, we proceed to perform the same calculations for the total population. As we are using many of the already defined variables, we will jump right into calculations of the population of Palau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the sum of the total population for each decade from 2010 to 2100 for the country of Palau. \n",
    "#This is done by performing a summation for all the cells determined as Palau with the matching cells \n",
    "#in the individual layers.\n",
    "\n",
    "tpop10 = np.sum(T['ssp4_2010'][Palau1])\n",
    "tpop20 = np.sum(T['ssp4_2020'][Palau1])\n",
    "tpop30 = np.sum(T['ssp4_2030'][Palau1])\n",
    "tpop40 = np.sum(T['ssp4_2040'][Palau1])\n",
    "tpop50 = np.sum(T['ssp4_2050'][Palau1])\n",
    "tpop60 = np.sum(T['ssp4_2060'][Palau1])\n",
    "tpop70 = np.sum(T['ssp4_2070'][Palau1])\n",
    "tpop80 = np.sum(T['ssp4_2080'][Palau1])\n",
    "tpop90 = np.sum(T['ssp4_2090'][Palau1])\n",
    "tpop100 = np.sum(T['ssp4_2100'][Palau1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the arrays for the plotting the total population\n",
    "\n",
    "Total = [tpop10,tpop20,tpop30,tpop40,tpop50,tpop60,tpop70,tpop80,tpop90,tpop100] #Our y-axis, containing the population\n",
    "Year = [2010,2020,2030,2040,2050,2060,2070,2080,2090,2100] #Our x-axis, defining the years\n",
    "plt.plot(Year,Total) \n",
    "plt.ylabel('Total Population') #Naming the y-axis\n",
    "plt.xlabel('Year') #Naming the x-axis\n",
    "plt.title('Palau') #Giving the chart a title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the two charts\n",
    "\n",
    "plt.plot(Year,Total) \n",
    "plt.plot(Year, Urban)\n",
    "plt.ylabel('Population') #Naming the y-axis\n",
    "plt.xlabel('Year') #Naming the x-axis\n",
    "plt.title('Palau') #Giving the chart a title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noted that only one line can be seen. The code has been debugged and tested with other countries. The plotting is correct - all cells on Palau are defined as Urban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1B ## \n",
    "Here we show the countries, which suffer a population decline from 2010 to 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop10 = T['ssp4_2010'] #Defining a variable for the total number of people in 2010\n",
    "Pop100 = T['ssp4_2100'] #Defining a variable for the total number of people in 2100\n",
    "\n",
    "popcomp = Pop10 - Pop100 #Calculating all the cells, which suffer a population decline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We proceed to plot a raster, showing all the cells, which suffer a decline.\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title('Population Decline 2010 - 2100')\n",
    "imgplot = plt.imshow(popcomp, norm=LogNorm(), cmap='hot_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2a\n",
    "Loading an shp file and a csv file, and performing a left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by importing the two files\n",
    "u5m = pd.read_csv('under5mortality.csv')\n",
    "world = gpd.read_file('ne_110m_admin_0_countries.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the left join, we use the world1 as our left table, and the under5mortality as our right side. We join on the ISO_A3 column from the left and the ISO column from the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Here we create the variable df, in which we perform the left join.  \n",
    "df = pd.merge(world, u5m, left_on='ISO_A3', right_on='ISO',  how='left') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2b\n",
    "Calculating the spatial weights matrix for the world, based on border neighbourhood, and using the weights to calculate Moran's I for child mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially we calculate the Queen's weight, where both shared edges and shared vertices lead to a point.\n",
    "w_queen = weights.Queen.from_dataframe(df) #Calculating on the df we created earlier\n",
    "pd.DataFrame(w_queen.full()[0], \n",
    "             index=df['ISO'],\n",
    "             columns=df['ISO'],\n",
    "            ).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the weights (1 or 0), we proceed to calculate Moran's I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = esda.Moran(df['ChildMortality'], w_queen)\n",
    "(moran.I, moran.p_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation of Moran's I returns NaN. This may be caused by the number being too small for Python to interpret.\n",
    "\n",
    "Alternatively, there is something wrong with the calculation. To test, we remove all rows, where ISO = NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = df.dropna(subset = ['ISO']) #We use the built-in function in Pandas \"DropNa\", to remove all rows where ISO = NaN. \n",
    "\n",
    "w_queenNaN = weights.Queen.from_dataframe(db) #Calculating on the df we created earlier\n",
    "pd.DataFrame(w_queenNaN.full()[0], \n",
    "             index=db['ISO'],\n",
    "             columns=db['ISO'],\n",
    "            ).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having removed all rows containing NaN in the ISO column, we calculate Moran's I again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moranNaN = esda.Moran(db['ChildMortality'], w_queenNaN)\n",
    "(moranNaN.I, moranNaN.p_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the test, it is possible to calculate Moran's I by removing all rows, in which ISO = NaN. Based on the above, we may safely reject the null-hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
